<!DOCTYPE HTML>
<html lang="en"><head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1Y9L74WH97"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
  
      gtag('config', 'G-1Y9L74WH97');
    </script>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zihan Wang's Homepage ÁéãÂ≠êÊ∂µ</title>
  
  <meta name="author" content="Zihan Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zihan Wang</name>
              </p>
              <p>
                I am a CS PhD student at 
		<a href="https://www.northwestern.edu/">Northwestern University</a>.
                I am fortunate to be advised by  
                <a href="https://limanling.github.io">Manling Li</a>.
                I got my bachelor's degree at 
                <a href="https://ai.ruc.edu.cn">GSAI</a>,
                <a href="https://www.ruc.edu.cn">Renmin University of China</a>. 
		I was a research intern at 
		<a href="http://blender.cs.illinois.edu">UIUC Blender Lab</a> and
                <a href="https://www.deepseek.com/">Deepseek</a>.
                I had a fantastic experience as an exchange student at 
                <a href="https://www.berkeley.edu/">
                  UC Berkeley</a>. 
              </p>
              <!-- <p style="color:red"><b>
                I'm excited to join Northwestern University starting from 2024 fall as a CS PhD student, advised by 
                <a href="https://limanling.github.io">Manling Li</a>!
              </b></p> -->
              <p style="text-align:center">
                <a href="https://www.overleaf.com/read/mxyjqgqvvrty">Resume</a>  &nbsp/&nbsp
                <!-- <a style="color:red" href="https://www.overleaf.com/read/dgwkwnknkprx#1bbeb2">Research Statement</a> -->
                <!-- &nbsp<br>&nbsp -->
                <a href="mailto:510642032wzh@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://twitter.com/wzihanw">X.com</a> &nbsp/&nbsp
                <a href="https://github.com/zihanwang314">Github</a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Zihan-Wang/2243360876">Semantic Scholar</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/platinum-47-88">Zhihu</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/photo.jpeg"><img
                      style="width:80%;max-width:80%" alt="profile photo" src="images/1011_profile.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>









        
        <heading>News</heading>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <ul>
			<li>
                      <b>Sep 20, 2024</b> - Glad to announce that
			<a href="https://arxiv.org/abs/2407.01906">ESFT</a>
			has been accepted to the EMNLP 2024 Main Conference! Many thanks to all collaborators!
                  </li>
                    <li>
                      <b>Jul 4, 2024</b> - Thrilled to introduce our latest project at Deepseek,
                      <a href="https://arxiv.org/abs/2407.01906">Expert-Specialized Fine-Tuning (ESFT)</a>
                       for efficient and effective LLM customization by leveraging the highly specialized Mixture-of-Experts (MoE) architecture!
                  </li>
                  <li><b>Feb 15, 2024</b> - Excited to join Northwestern as a PhD student! Many thanks to my advisor <a href="https://limanling.github.io">Manling Li</a>!
                  </li> 
                  <li><b>Oct 19, 2023</b> - Honored to be awarded the 
                    <a href="https://mp.weixin.qq.com/s/Nb6RIBYcZQp_K66wvEr34A"> Baosteel Outstanding Student Award 2023 </a>
                    as the <b>ONLY</b> undergrad student among science and technology departments in RUC! Special thanks to 
                    <a href="http://playbigdata.ruc.edu.cn/dou">NLPIR lab</a>
                    !
                  </li>
                  <!-- <li><b>Sep 20, 2023</b> - Glad to announce that I am currently working on a brand-new topic of 
                    fine-grained control on interactive language models with
                    <a href="https://wyshi.github.io">Dr. Weiyan Shi</a> from Stanford University! Paper will come out soon!
                  </li> -->
                  <!-- </li>
                  <li><b>Sep, 2023</b> - I'm currently quite obsessed with <b>scalable oversight on interactive language models</b>. 
                    Welcome to discuss ideas with me. Please drop an e-mail if interested.
                  </li> -->
                  <li><b>Jun 7, 2023</b> - Excited to share that I'll be joining 
                    <a href="https://blender.cs.illinois.edu/">UIUC Blender Lab</a>
                    this summer as a student researcher!
                  </li>
                  <li><b>Mar 15, 2023</b> - My talk on <font size="3">LARGE</font> language models for 
                    <a href="https://cosx.org/">Capital of Statistics</a>
                    will take place at 7:00 PM Mar 17, 2023 BJT! Click
                    <a href="https://mp.weixin.qq.com/s/9G8wct4ktTHqQw7FFFhbQg">here</a>
                    for more details. (Update: <a href="./data/LLMs.pptx" download> slides</a>, <a href="https://www.bilibili.com/video/BV1EL411C79J">video</a>)
                  </li>
                  <li><b>Jan 12, 2023</b> - I will give a talk on pre-trained models and their applications
                    at 2:00 PM Jan 13, 2023 BJT for 
                    <a href="http://mlc.ruc.edu.cn/">Mingli College</a>! For more information, click
                    <a href="https://mp.weixin.qq.com/s/ecaswp2h04tCgL7O4B5RGQ">here</a>.
                    (Update: <a href="./data/pretraining.pptx" download> slides</a>)
                  </li>
                  <li><b>Dec 12, 2022</b> - I posted an article introducing ChatGPT on 
                    <a href="https://cosx.org/">Capital of Statistics</a>. 
                    Do not miss it if you want to know more about ChatGPT! 
                    (Only Chinese version available currently)
                    (<a href="https://mp.weixin.qq.com/s/JiWpTORpjqzOgC8IsslKVA">link</a>)
                  </li>
                </ul>
            </td>
          </tr>
        </tbody></table>

        <heading>Research Interest</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                  I work on a wide range topic regarding Large Language Models.
                  I have done several works regarding (1) general interaction, e.g., <a href="https://xingyaoww.github.io/mint-bench/">MINT interaction benchmark</a>, (2) the cross application of language models and information retrieval (IR) systems, e.g., <a href="https://arxiv.org/pdf/2306.05212.pdf">retrieval augmented models (RetaLLM)</a> and <a href="pdf/novo.pdf">LM-based IR</a>, (4) efficient alignment of LLMs, e.g., <a href="https://arxiv.org/abs/2407.01906">expert-specialized fine-tuning</a>.                
              </p>
            </td>
          </tr>
        </tbody></table>

        






	<heading>Selected Publications</heading>
	<p>See full list on <a href="https://www.semanticscholar.org/author/Zihan-Wang/2243360876" target="_blank">Semantic Scholar</a></p>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/esft.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models</papertitle>
                <br>
                <strong>Zihan Wang</strong>,
                <a href="https://victorchen96.github.io/chendeli.io/">Deli Chen</a>,
                <a href="https://scholar.google.com.hk/citations?user=8b-ysf0NWVoC&hl=zh-CN">Damai Dai</a>,
                <a href="https://runxinxu.github.io/aboutme/">Runxin Xu</a>,
                <a href="http://www.idi.zju.edu.cn/member/3053.html">Zhuoshu Li</a>,
                Y. Wu
                <br>
                <em>EMNLP 2024</em>
                <br>
                <a href="pdf/esft.pdf">[paper]</a>
                <a href="https://github.com/deepseek-ai/ESFT">[code]</a>
                <p>
                  We propose Expert-Specialized Fine-Tuning (ESFT) to customize Large Language Models (LLMs) with Mixture-of-Experts (MoE) architecture by adjusting only task-relevant parts, improving efficiency and performance while using fewer resources and storage. We found that different experts in MoE models specialize in specific tasks, making training these task-relevant experts more cost-effective than training shared parameters.
                </p>
                <br>
                </td>
            </tr>

            
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MINT.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback</papertitle>
              <br>
              <a href="https://xingyaoww.github.io">Xingyao Wang*</a>,
              <strong>Zihan Wang*</strong>,
              <a href="https://lumos-jiateng.github.io">Jiateng Liu</a>,
              <a href="https://yangyi-chen.github.io">Yangyi Chen</a>,
              <a href="https://lifan-yuan.github.io">Lifan Yuan</a>,
              <a href="https://haopeng-nlp.github.io/">Hao Peng</a>,
              <a href="http://blender.cs.illinois.edu/hengji.html">Heng Ji</a>
              <br>
              <em>ICLR 2024</em>
              <br>
              <a href="pdf/mint.pdf">[paper]</a>
              <a href="https://github.com/xingyaoww/mint-bench">[code]</a>
              <a href="https://xingyaoww.github.io/mint-bench">[website]</a>
              <p>
                Current LLM evaluations focus on single-turn and overlook multi-turn real-world scenarios. 
                We introduce the MINT benchmark to assess LLMs in interaction with tools and language feedback. 
                Our study of 20 LLMs shows they benefit from multi-turn interactions, but current RLHF and SIFT methods might hinder this.
                MINT aims to encourage research on LLM multi-turn capabilities, especially in open-source models.
              </p>
              <br>
              </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/NOVO.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>NOVO: Learnable and Interpretable Document Identifiers for Model Based IR</papertitle>
              <br>
              <strong>Zihan Wang</strong>,
              <a href="https://www.zhouyujia.cn/">Yujia Zhou</a>,
              Yiteng Tu, 
              <a href="http://playbigdata.ruc.edu.cn/dou">Zhicheng Dou</a>.
              <br>
              <em>CIKM 2023, <b>Oral Presentation</b></em>
              <br>
              <a href="pdf/cikm23.pdf">[paper]</a>
              <a href="">[code]</a>
              <p> 
                We propose Neural Optimized Vocabularial doc-ids for model-based information retrieval. 
                NOVO doc-ids consist of non-overlapping n-gram sets to identify documents,
                optimized through query denoising modeling and retrieval tasks.
                We further demonstrate how N-grams lead to interpretability in understanding documents and queries. 
              </p>
              <br>
              </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/retallm.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>RetaLLM: A Retrieval-Augmented Large Language Model Toolkit</papertitle>
              <br>
              <a href="https://github.com/rucliujn">Jiongnan Liu</a>,
              <a href="https://github.com/ignorejjj">Jiajie Jin</a>,
              <strong>Zihan Wang</strong>, 
              <a href="https://paperswithcode.com/search?q=author%3AJiehan+Cheng">Jiehan Cheng</a>,
              <a href="http://playbigdata.ruc.edu.cn/dou">Zhicheng Dou</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=tbxCHJgAAAAJ">Ji-Rong Wen</a>
              <br>
              <em>Arxiv Preprint</em>
              <br>
							<a href="pdf/reta.pdf">[paper]</a>
              <a href="https://github.com/ruc-gsai/yulan-ir">[code]</a>
              <p> 
                We develop a RETreival-Augmented LLM toolkit. 
                It provides more plug-and-play modules to support better interaction between IR systems and LLMs,
                including request rewriting, document retrieval, passage extraction, answer generation, and fact checking modules.      
              </p>
              </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/CQA.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Learning on Structured Documents for Conditional Question Answering</papertitle>
              <br>
              <strong>Zihan Wang</strong>, 
              <a href="https://qhjqhj00.github.io/">Hongjin Qian</a>, 
              <a href="http://playbigdata.ruc.edu.cn/dou">Zhicheng Dou</a>
              <br>
              <em>CCL 2023</em>
              <br>
							<a href="pdf/lsd.pdf">[paper]</a><a href="">[code]</a><a href="data/ccl-2023-poster.pdf">[poster]</a>
              <p>
                We propose a self-supervised learning method on structured documents for conditional question answering,
                consisting of a conditional question generation approach and a contrastive learning objective.
                We propose a pipeline able to generate multiple answers with detailed conditions.       
              </p>
              </td>
          </tr>

        </tbody></table>











        <heading>Misc</heading>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
                <ul>
                    <li> I like to work and chat with people from diverse backgrounds (üåà), which I believe is the key to true innovation. Feel free to contact me. </li>
                    <li>I love Sandbox games like Minecraft and Danmaku games like Touhou Project. 
                      I also loved designing RPG games when I was in primary school (with 
                      <a href="https://www.rpgmakerweb.com/products/rpg-maker-xp">RMXP</a> on WindowsXP), although
                      they cannot be launched anymore on Win10.
                    </li>
                    <li>My dream was to be a vlogger and I posted 
                      <a href="https://space.bilibili.com/2066036?spm_id_from=333.337.0.0">videos</a>
                      on bilibili, including vlogs, game playing records and some parody videos. 
                    </li>
                    <li>Besides Chinese and English, I can speak a little Japanese due to my passion in Anime
                      in my childhood. My favorite Anime was „ÉØ„É≥„Éî„Éº„Çπ and Fate/stay night.
                    </li>
                    
                </ul>

            </td>
          </tr>


        </tbody></table>

        <heading>Awards</heading>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <ul>
                  <li><b>Baosteel Outstanding Student Award</b>, 7/30000+, Renmin Univ. of China, 2023</li>
                  <li><b>First Class Academic Excellence Award</b> (top 3% GPA), Renmin Univ. of China, 2021</li>
                  <li><b>Provincal First Prize</b>, Contemporary Undergraduate Mathematical Contest in
                    Modeling, 2021</li>
                  <li><b>Honorable Mention</b>, Mathematical Contest in Modeling and Interdisciplinary
                    Contest in Modeling, 2021</li>
                </ul>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website design from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
    <tr style="padding:0px">
      <td style="padding:2.5%;width:40%;vertical-align:middle">
        <div class="page__footer">
          <footer> <!-- start custom footer snippets --> 
            <a href="/sitemap/">Sitemap</a> 
            <!-- end custom footer snippets -->
            <div class="page__footer-copyright">
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=o89zY8a0ARtuHIK-X34murs3PrRBFGrB1jTX7j2kYt8&cl=ffffff&w=a"></script>
            </div>
          </footer>
        </div>
      </td>
    </tr>
  </tbody></table>


  
</body>

</html>
