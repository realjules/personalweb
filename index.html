<!DOCTYPE HTML>
<html lang="en"><head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1Y9L74WH97"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
  
      gtag('config', 'G-1Y9L74WH97');
    </script>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zihan Wang's Homepage ÁéãÂ≠êÊ∂µ</title>
  
  <meta name="author" content="Zihan Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
	
	<style>
	  /* Style for the floating window */
	  #floatWindow {
	      display: none; /* Hidden by default */
	      position: absolute;
	      background-color: #f9f9f9;
	      border: 1px solid #ccc;
	      padding: 15px;
	      width: 300px;
	      box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2);
	      z-index: 100;
	      top: 100px;
	      left: 100px;
	  }
	
	  /* Close button style */
	  #closeBtn {
	      display: inline-block;
	      float: right;
	      cursor: pointer;
	      color: #aaa;
	  }
	
	  #closeBtn:hover {
	      color: #000;
	  }
	</style>

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zihan Wang</name>
              </p>
              <p>
                I am a first-year CS PhD student at 
		<a href="https://www.northwestern.edu/">Northwestern University</a>,
                fortunate to be advised by the wonderful
                <a href="https://limanling.github.io">Manling Li</a>.
                I got my bachelor's degree at 
                <a href="https://ai.ruc.edu.cn">Gaoling School of AI</a>, RUC.
		I have also had the privilege to work with  
		<a href="http://blender.cs.illinois.edu">Heng Ji</a>
		at UIUC and collaborate with fantastic teams at
                <a href="https://www.deepseek.com/">Deepseek</a>.
              </p>
              <!-- <p style="color:red"><b>
                I'm excited to join Northwestern University starting from 2024 fall as a CS PhD student, advised by 
                <a href="https://limanling.github.io">Manling Li</a>!
              </b></p> -->
              <p style="text-align:center">
                <a href="https://www.overleaf.com/read/mxyjqgqvvrty">CV</a>  &nbsp/&nbsp
                <!-- <a style="color:red" href="https://www.overleaf.com/read/dgwkwnknkprx#1bbeb2">Research Statement</a> -->
                <!-- &nbsp<br>&nbsp -->
                <a href="mailto:510642032wzh@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://github.com/zihanwang314">Github</a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Zihan-Wang/2243360876">Semantic Scholar</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/platinum-47-88">Zhihu</a> 
		<br>
		<a href="https://twitter.com/wzihanw?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @wzihanw</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/photo.jpeg"><img
                      style="width:80%;max-width:80%" alt="profile photo" src="images/1011_profile.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>









        
        <heading>News</heading>
	<table width="100%" align="center" border="0" cellpadding="20">
	  <tbody>
	    <tr>
	      <td style="padding:20px;width:75%;vertical-align:middle">
	        <ul>
	          <li>
	            <b>üóìÔ∏è Sep 20, 2024</b> - Glad to announce that
	            <a href="https://arxiv.org/abs/2407.01906">ESFT</a>
	            has been accepted to the EMNLP 2024 Main Conference! üéâ Many thanks to all collaborators!
	          </li>
	          <li>
	            <b>üóìÔ∏è Jul 4, 2024</b> - Thrilled to introduce our latest project at Deepseek,
	            <a href="https://arxiv.org/abs/2407.01906">Expert-Specialized Fine-Tuning (ESFT)</a> for efficient and effective LLM customization by leveraging the highly specialized Mixture-of-Experts (MoE) architecture! ü§ñ‚ú®
	          </li>
	          <li><b>üóìÔ∏è Feb 15, 2024</b> - Excited to join Northwestern as a PhD student! üéì Many thanks to my advisor <a href="https://limanling.github.io">Manling Li</a>!
	          </li>
	          <li><b>üóìÔ∏è Oct 19, 2023</b> - Honored to be awarded the 
	            <a href="https://mp.weixin.qq.com/s/Nb6RIBYcZQp_K66wvEr34A">Baosteel Outstanding Student Award 2023</a> üèÖ as the <b>ONLY</b> undergrad student among science and technology departments in RUC! Special thanks to 
	            <a href="http://playbigdata.ruc.edu.cn/dou">NLPIR lab</a>! üôè
	          </li>
	          <li><b>üóìÔ∏è Jun 7, 2023</b> - Excited to share that I'll be joining 
	            <a href="https://blender.cs.illinois.edu/">UIUC Blender Lab</a> üî¨ this summer as a student researcher!
	          </li>
	          <li><b>üóìÔ∏è Mar 15, 2023</b> - My talk on <font size="3">LARGE</font> language models at 
	            <a href="https://cosx.org/">Capital of Statistics</a> üìä will take place at 7:00 PM Mar 17, 2023 BJT! Click
	            <a href="https://mp.weixin.qq.com/s/9G8wct4ktTHqQw7FFFhbQg">here</a> for more details. (Update: <a href="./data/LLMs.pptx" download>slides</a>, <a href="https://www.bilibili.com/video/BV1EL411C79J">video</a>)
	          </li>
	          <li><b>üóìÔ∏è Jan 12, 2023</b> - I will give a talk on pre-trained models and their applications üìö at 2:00 PM Jan 13, 2023 BJT at 
	            <a href="http://mlc.ruc.edu.cn/">Mingli College</a>! For more information, click
	            <a href="https://mp.weixin.qq.com/s/ecaswp2h04tCgL7O4B5RGQ">here</a>.
	            (Update: <a href="./data/pretraining.pptx" download>slides</a>)
	          </li>
	          <li><b>üóìÔ∏è Dec 12, 2022</b> - I posted an article introducing ChatGPT on 
	            <a href="https://cosx.org/">Capital of Statistics</a> üí°. Do not miss it if you want to know more about ChatGPT! (<a href="https://mp.weixin.qq.com/s/JiWpTORpjqzOgC8IsslKVA">link</a>)
	          </li>
	        </ul>
	      </td>
	    </tr>
	  </tbody>
	</table>
	      
        <heading>Research Interest</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                  I work on various topics regarding Large Language Models, including <b> interaction, alignment, and long-context understanding (retrieval, IR).</b>
                  My representative works include 
		      (1) general interaction, e.g., 
		      	<a href="https://xingyaoww.github.io/mint-bench/">MINT interaction benchmark</a>, 
		      (2) the cross-application of LLM & IR, e.g., 
		      	<a href="https://arxiv.org/pdf/2306.05212.pdf">retrieval augmented models (RetaLLM)</a> 
		      	and <a href="pdf/novo.pdf">LM-based IR</a>, 
		      (3) efficient alignment of LLMs, e.g., 
		      	<a href="https://arxiv.org/abs/2407.01906">expert-specialized fine-tuning</a>.                
              </p>
            </td>
          </tr>
        </tbody></table>

        





	<heading>Selected Publications</heading>
	<p>See full list on <a href="https://www.semanticscholar.org/author/Zihan-Wang/2243360876" target="_blank">Semantic Scholar</a> (<a href="#" id="openFloat">Why I Love Semantic Scholar, and You Might Too</a>)</p> 
	<!-- The floating window -->
	<div id="floatWindow">
	    <span id="closeBtn">&times;</span>
	    <p>
        Semantic Scholar uses <b>AI-powered</b> tools to summarize papers, highlight key phrases, and rank research by influence.
        This helps you <b>find important studies faster</b>. 
        Its <b> Semantic Reader </b> helps you understand papers with <b> skimming highlights </b> and <b> citation cards </b>.
        You can also see how papers connect with citation graphs. 
        While Google Scholar is great for broad searches, 
        Semantic Scholar is smarter for finding high-quality and impactful research!
	    </p>
	</div>
	      

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/esft.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
		<papertitle><span style="color:red;">[New]</span> Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models</papertitle>
                <br>
                <strong>Zihan Wang</strong>,
                <a href="https://victorchen96.github.io/chendeli.io/">Deli Chen</a>,
                <a href="https://scholar.google.com.hk/citations?user=8b-ysf0NWVoC&hl=zh-CN">Damai Dai</a>,
                <a href="https://runxinxu.github.io/aboutme/">Runxin Xu</a>,
                <a href="http://www.idi.zju.edu.cn/member/3053.html">Zhuoshu Li</a>,
                <a href="https://scholar.google.com/citations?user=aQizmzsAAAAJ&hl=zh-CN"> Yu Wu </a>
                <br>
                <em>EMNLP 2024</em>
                <br>
		<a href="https://arxiv.org/abs/2407.01906" style="vertical-align: middle;">[paper]</a>
		<a href="https://github.com/deepseek-ai/ESFT" style="vertical-align: middle;">[code]</a>
		<img alt="" src="https://img.shields.io/github/stars/deepseek-ai/ESFT?style=social" style="vertical-align: middle;">
                <p>
                  We harness the <b> Specialized Power of Experts </b> in MoE LLMs through ESFT. By fine-tuning <b> Down to 5% Experts </b> in a layer, near-full performance can be achieved. 
                </p>
                <br>
                </td>
            </tr>

            
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/MINT.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><span style="color:red;">[Highlight]</span> MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback</papertitle>
              <br>
              <a href="https://xingyaoww.github.io">Xingyao Wang*</a>,
              <strong>Zihan Wang*</strong>,
              <a href="https://lumos-jiateng.github.io">Jiateng Liu</a>,
              <a href="https://yangyi-chen.github.io">Yangyi Chen</a>,
              <a href="https://lifan-yuan.github.io">Lifan Yuan</a>,
              <a href="https://haopeng-nlp.github.io/">Hao Peng</a>,
              <a href="http://blender.cs.illinois.edu/hengji.html">Heng Ji</a>
              <br>
              <em>ICLR 2024</em>
              <br>
		<a href="pdf/mint.pdf" style="vertical-align: middle;">[paper]</a>
		<a href="https://xingyaoww.github.io/mint-bench" style="vertical-align: middle;">[website]</a>
		<a href="https://github.com/xingyaoww/mint-bench" style="vertical-align: middle;">[code]</a>
		<img alt="" src="https://img.shields.io/github/stars/xingyaoww/mint-bench?style=social" style="vertical-align: middle;">
              <p>
		We introduce <b>MINT</b>, a benchmark for evaluating LLMs in <b>Multi-turn Interactions</b> with tools and language feedback. 
                MINT reveals several limitations in existing RLHF and SIFT methods on multi-turn interaction.
              </p>
              <br>
              </td>
          </tr>

	  <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/activationparameters.png?raw=true' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
		<papertitle><span style="color:red;">[Highlight]</span> DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</papertitle>
                <br>
		<a href="https://chat.deepseek.com"> DeepSeeek AI</a> (157 authors including <strong> Zihan Wang</strong>)
                <br>
		<a href="https://arxiv.org/abs/2405.04434" style="vertical-align: middle;">[paper]</a>
		<a href="https://github.com/deepseek-ai/DeepSeek-V2?tab=readme-ov-file" style="vertical-align: middle;">[code]</a>
		<img alt="" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2?style=social" style="vertical-align: middle;">
                <p>
		DeepSeek-V2 is a strong MoE model with 23B activated parameters. It achieves stronger 
		performance compared to DeepSeek 67B, <b>saving 42.5% training costs and boosting generation by up to 5.76x</b>.
                </p>
                <br>
                </td>
            </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/NOVO.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>NOVO: Learnable and Interpretable Document Identifiers for Model Based IR</papertitle>
              <br>
              <strong>Zihan Wang</strong>,
              <a href="https://www.zhouyujia.cn/">Yujia Zhou</a>,
              Yiteng Tu, 
              <a href="http://playbigdata.ruc.edu.cn/dou">Zhicheng Dou</a>.
              <br>
              <em>CIKM 2023, <b>Oral Presentation</b></em>
              <br>
		<a href="pdf/cikm23.pdf" style="vertical-align: middle;">[paper]</a>
		<a href="https://github.com/ZihanWang314/NOVO" style="vertical-align: middle;">[code]</a>
		<img alt="" src="https://img.shields.io/github/stars/zihanwang314/NOVO?style=social" style="vertical-align: middle;">
              <p> 
                We propose learnable NOVO doc-ids for model-based IR. 
                NOVO doc-ids consist of non-overlapping n-gram sets to identify documents,
                optimized through denoising queries and retrieval tasks.
              </p>
              <br>
              </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/retallm.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>RetaLLM: A Retrieval-Augmented Large Language Model Toolkit</papertitle>
              <br>
              <a href="https://github.com/rucliujn">Jiongnan Liu</a>,
              <a href="https://github.com/ignorejjj">Jiajie Jin</a>,
              <strong>Zihan Wang</strong>, 
              <a href="https://paperswithcode.com/search?q=author%3AJiehan+Cheng">Jiehan Cheng</a>,
              <a href="http://playbigdata.ruc.edu.cn/dou">Zhicheng Dou</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=tbxCHJgAAAAJ">Ji-Rong Wen</a>
              <br>
		<a href="pdf/reta.pdf" style="vertical-align: middle;">[paper]</a>
		<a href="https://github.com/ruc-gsai/yulan-ir" style="vertical-align: middle;">[code]</a>
		<img alt="" src="https://img.shields.io/github/stars/ruc-gsai/yulan-ir?style=social" style="vertical-align: middle;">
		
              <p> 
                We develop a Retreival-Augmented LLM toolkit for better interaction between LLMs and retrieval systems.
                Feature modules: request rewriting, passage extraction, and fact-checking.      
              </p>
              </td>
          </tr>

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/CQA.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Learning on Structured Documents for Conditional Question Answering</papertitle>
              <br>
              <strong>Zihan Wang</strong>, 
              <a href="https://qhjqhj00.github.io/">Hongjin Qian</a>, 
              <a href="http://playbigdata.ruc.edu.cn/dou">Zhicheng Dou</a>
              <br>
              <em>CCL 2023</em>
              <br>
			<a href="pdf/lsd.pdf">[paper]</a><a href="data/ccl-2023-poster.pdf">[poster]</a>
              <p>
                We propose self-supervised learning on structured documents for conditional QA.
                It consists of a conditional question generation approach and a contrastive learning objective.
                We propose a two-stage pipeline to generate conditional answers. 
              </p>
              </td>
          </tr> -->

        </tbody></table>



        
        <heading>Professional Service</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
	    	<ul>
			  <li><b>Program Committee (Reviewer) - </b> <a href="https://2024.emnlp.org/calls/demos/">EMNLP 2024</a></li>
			  <li><b>Session Organization - </b> Session of Language Models & Agents in <a href="https://china-r.cosx.org/bj2023/lectures.html">Chinese R Conference 2023</a></li>
			  <li><b>Academic Advisor - </b> <a href="http://ai.ruc.edu.cn/newslist/notice/20230411001.html">National University Student Innovation Program (2023, No. 3)</a></li>                  
                </ul>
            </td>
          </tr>
        </tbody></table>

        <heading>Invited Talks and Presentations</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
	    	<ul>
		  <li>
		    <a href="https://china-r.cosx.org/bj2023/lectures.html">LLM Agents with Language Feedback</a>, 
		    Chinese R Conference &nbsp;&nbsp;&nbsp;&nbsp; 2023.11
		  </li>
		  <li>
		    <a href="http://ai.ruc.edu.cn/newslist/newsdetail/20230606002.html">Retrieval Augmented Language Models and Applications</a>,
		    RUC Science and Technology Fair &nbsp;&nbsp;&nbsp;&nbsp; 2023.05
		  </li>
		  <li>
		    <a href="https://www.bilibili.com/video/BV1EL411C79J/?spm_id_from=333.337.search-card.all.click&vd_source=ad803afa85e584a3fc4cbfa14cca9a57">Large Language Models and Applications</a>,
		    Capital of Statistics &nbsp;&nbsp;&nbsp;&nbsp; 2023.03
		  </li>
		  <li>
		    <a href="https://mp.weixin.qq.com/s/ecaswp2h04tCgL7O4B5RGQ">Pre-trained Language Models and Applications</a>,
		    RUC Mingli College &nbsp;&nbsp;&nbsp;&nbsp; 2023.01
		  </li>
		</ul>
            </td>
          </tr>
        </tbody></table>


        







        <heading>Misc</heading>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
                <ul>
                    <li> I like to work and chat with people from diverse backgrounds (üåà), which I believe is the key to true innovation. Feel free to contact me. </li>
                    <li>I love Sandbox games like Minecraft and Danmaku games like Touhou Project. 
                      I also loved designing RPG games when I was in primary school (with 
                      <a href="https://www.rpgmakerweb.com/products/rpg-maker-xp">RMXP</a> on WindowsXP), although
                      they cannot be launched anymore on Win10.
                    </li>
                    <li>My dream was to be a vlogger and I posted 
                      <a href="https://space.bilibili.com/2066036?spm_id_from=333.337.0.0">videos</a>
                      on bilibili, including vlogs, game playing records and some parody videos. 
                    </li>
                    <li>Besides Chinese and English, I can speak a little Japanese due to my passion in Anime
                      in my childhood. My favorite Anime was „ÉØ„É≥„Éî„Éº„Çπ and Fate/stay night.
                    </li>
                    
                </ul>

            </td>
          </tr>


        </tbody></table>

        <heading>Awards</heading>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <ul>
                  <li><b>Baosteel Outstanding Student Award</b>, 7/30000+, Renmin Univ. of China, 2023</li>
                  <li><b>First Class Academic Excellence Award</b> (top 3% GPA), Renmin Univ. of China, 2021</li>
                  <li><b>Provincal First Prize</b>, Contemporary Undergraduate Mathematical Contest in
                    Modeling, 2021</li>
                  <li><b>Honorable Mention</b>, Mathematical Contest in Modeling and Interdisciplinary
                    Contest in Modeling, 2021</li>
                </ul>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website design from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
    <tr style="padding:0px">
      <td style="padding:2.5%;width:40%;vertical-align:middle">
        <div class="page__footer">
          <footer> <!-- start custom footer snippets --> 
            <a href="/sitemap/">Sitemap</a> 
            <!-- end custom footer snippets -->
            <div class="page__footer-copyright">
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=o89zY8a0ARtuHIK-X34murs3PrRBFGrB1jTX7j2kYt8&cl=ffffff&w=a"></script>
            </div>
          </footer>
        </div>
      </td>
    </tr>
  </tbody></table>

  <script>
    // Get the elements
    const floatWindow = document.getElementById('floatWindow');
    const openFloat = document.getElementById('openFloat');
    const closeBtn = document.getElementById('closeBtn');
  
    // Open the float window when the link is clicked
    openFloat.addEventListener('click', function(event) {
        event.preventDefault(); // Prevent the default action of the link
  
        // Get the position of the link element
        const rect = openFloat.getBoundingClientRect();
  
        // Position the floating window near the link
        floatWindow.style.top = rect.top + window.scrollY + 30 + 'px'; // 30px below the link
        floatWindow.style.left = rect.left + window.scrollX + 'px'; // Align it with the link horizontally
  
        // Display the floating window
        floatWindow.style.display = 'block';
    });
  
    // Close the float window when the close button is clicked
    closeBtn.addEventListener('click', function() {
        floatWindow.style.display = 'none';
    });
  
    // Close the float window when clicking outside of it
    window.addEventListener('click', function(event) {
        if (event.target !== floatWindow && event.target !== openFloat && !floatWindow.contains(event.target)) {
            floatWindow.style.display = 'none';
        }
    });
  </script>
  

  
</body>

</html>
